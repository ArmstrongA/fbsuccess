{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArmstrongA/fbsuccess/blob/main/optimized_scrapper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RD4KAhMEJc_Y"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "import os\n",
        "import time\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2zZFoFBJc_h",
        "outputId": "0e75d74b-112d-470b-8ced-bd2a92bad0c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 2236 matches...\n"
          ]
        }
      ],
      "source": [
        "# Full stats for fbref.com seem to start in 2017\n",
        "epl = [\"https://fbref.com/en/comps/9/schedule/Premier-League-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/9/2021-2022/schedule/2021-2022-Premier-League-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/9/2020-2021/schedule/2020-2021-Premier-League-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/9/2019-2020/schedule/2019-2020-Premier-League-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/9/2018-2019/schedule/2018-2019-Premier-League-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/9/2017-2018/schedule/2017-2018-Premier-League-Scores-and-Fixtures\"]\n",
        "\n",
        "epl_current = [\"https://fbref.com/en/comps/9/schedule/Premier-League-Scores-and-Fixtures\"]\n",
        "\n",
        "ligue1 = [\"https://fbref.com/en/comps/13/schedule/Ligue-1-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/13/2021-2022/schedule/2021-2022-Ligue-1-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/13/2020-2021/schedule/2020-2021-Ligue-1-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/13/2019-2020/schedule/2019-2020-Ligue-1-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/13/2018-2019/schedule/2018-2019-Ligue-1-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/13/2017-2018/schedule/2017-2018-Ligue-1-Scores-and-Fixtures\"]\n",
        "\n",
        "\n",
        "ligue1_current = [\"https://fbref.com/en/comps/13/schedule/Ligue-1-Scores-and-Fixtures\"]\n",
        "\n",
        "bundesliga = [\"https://fbref.com/en/comps/20/schedule/Bundesliga-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/20/2021-2022/schedule/2021-2022-Bundesliga-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/20/2020-2021/schedule/2020-2021-Bundesliga-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/20/2019-2020/schedule/2019-2020-Bundesliga-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/20/2018-2019/schedule/2018-2019-Bundesliga-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/20/2017-2018/schedule/2017-2018-Bundesliga-Scores-and-Fixtures\"]\n",
        "\n",
        "bundesliga_current = [\"https://fbref.com/en/comps/20/schedule/Bundesliga-Scores-and-Fixtures\"]\n",
        "\n",
        "seriea = [\"https://fbref.com/en/comps/11/schedule/Serie-A-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/11/2021-2022/schedule/2021-2022-Serie-A-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/11/2020-2021/schedule/2020-2021-Serie-A-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/11/2019-2020/schedule/2019-2020-Serie-A-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/11/2018-2019/schedule/2018-2019-Serie-A-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/11/2017-2018/schedule/2017-2018-Serie-A-Scores-and-Fixtures\"]\n",
        "\n",
        "seriea_current = [\"https://fbref.com/en/comps/11/schedule/Serie-A-Scores-and-Fixtures\"]\n",
        "\n",
        "laliga = [\"https://fbref.com/en/comps/12/schedule/La-Liga-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/12/2021-2022/schedule/2021-2022-La-Liga-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/12/2020-2021/schedule/2020-2021-La-Liga-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/12/2019-2020/schedule/2019-2020-La-Liga-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/12/2018-2019/schedule/2018-2019-La-Liga-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/12/2017-2018/schedule/2017-2018-La-Liga-Scores-and-Fixtures\"]\n",
        "\n",
        "laliga_current = [\"https://fbref.com/en/comps/12/schedule/La-Liga-Scores-and-Fixtures\"]\n",
        "\n",
        "uefa_current = [\"https://fbref.com/en/comps/8/schedule/Champions-League-Scores-and-Fixtures\"]\n",
        "\n",
        "uefa = [\"https://fbref.com/en/comps/8/schedule/Champions-League-Scores-and-Fixtures\",\n",
        "        \"https://fbref.com/en/comps/8/2021-2022/schedule/2021-2022-Champions-League-Scores-and-Fixtures\",\n",
        "        \"https://fbref.com/en/comps/8/2020-2021/schedule/2020-2021-Champions-League-Scores-and-Fixtures\",\n",
        "        \"https://fbref.com/en/comps/8/2019-2020/schedule/2019-2020-Champions-League-Scores-and-Fixtures\",\n",
        "        \"https://fbref.com/en/comps/8/2018-2019/schedule/2018-2019-Champions-League-Scores-and-Fixtures\",\n",
        "        \"https://fbref.com/en/comps/8/2017-2018/schedule/2017-2018-Champions-League-Scores-and-Fixtures\"]\n",
        "\n",
        "europa = [\"https://fbref.com/en/comps/19/2022/schedule/2022-Scores-and-Fixtures\",\n",
        "        \"https://fbref.com/en/comps/19/2021-2022/schedule/2021-2022-Europa-League-Scores-and-Fixtures\",\n",
        "        \"https://fbref.com/en/comps/19/2020-2021/schedule/2020-2021-Europa-League-Scores-and-Fixtures\",\n",
        "        \"https://fbref.com/en/comps/19/2019-2020/schedule/2019-2020-Europa-League-Scores-and-Fixtures\",\n",
        "        \"https://fbref.com/en/comps/19/2018-2019/schedule/2018-2019-Europa-League-Scores-and-Fixtures\",\n",
        "        \"https://fbref.com/en/comps/19/2017-2018/schedule/2017-2018-Europa-League-Scores-and-Fixtures\"]\n",
        "\n",
        "\n",
        "# Scrape one league at a time\n",
        "\n",
        "sched_pages = ligue1\n",
        "name = 'ligue1'\n",
        "\n",
        "match_pages = set()\n",
        "for s in sched_pages:\n",
        "    html = requests.get(s).text\n",
        "    match_url_regex = \"\\/en\\/matches\\/.{8}\\/[^\\\"]+\"\n",
        "    matches = re.findall(match_url_regex, html)\n",
        "    match_pages.update(matches)\n",
        "print(f\"found {len(match_pages)} matches...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google drive"
      ],
      "metadata": {
        "id": "O77ge_hq8FwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "er9jPZwW8Evv",
        "outputId": "3cda887e-cc3e-4244-959f-0612a25566dd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KPSxGgSoJc_n"
      },
      "outputs": [],
      "source": [
        "\n",
        "def parse_match(html, url):\n",
        "    soup = BeautifulSoup(html, 'lxml')\n",
        "    match = {}\n",
        "    \n",
        "    # get game meta data\n",
        "    league = ' '.join(re.findall('[a-zA-Z]+', url.split('-')[-2]))\n",
        "    match['match_url'] = url\n",
        "    match['league_id'] = league\n",
        "    match['game_id'] = url.split('/')[-2]\n",
        "    match['date'], match['game_time'] = soup.select_one('span.venuetime')['data-venue-date'], soup.select_one('span.venuetime')['data-venue-time']\n",
        "    \n",
        "    # get team names\n",
        "    title = soup.title.string.split(' Report')[0].replace('.', '').replace(' ', '_')\n",
        "    title = title.split('_Match')[0]\n",
        "    team_names = title.split(\"_vs_\")\n",
        "    home_team, away_team = team_names[0].replace('_', \" \"), team_names[1].replace('_', \" \")\n",
        "    match['match'], match['home_team'], match['away_team'] = title, home_team, away_team\n",
        "    \n",
        "    # get scores and xG\n",
        "    scorebox = soup.select_one('div.scorebox')\n",
        "    match['home_score'], match['away_score'] = scorebox.select_one('div.score').text.strip(), scorebox.select('div.score')[1].text.strip()\n",
        "    match['home_score_xg'], match['away_score_xg'] = scorebox.select_one('div.score_xg').text.strip(), scorebox.select('div.score_xg')[1].text.strip()\n",
        "    \n",
        "    # get stats\n",
        "    stat_table_regexes = [\n",
        "        'stats_.+_passing$',\n",
        "        'stats_.+_passing_types',\n",
        "        'stats_.+_defense',\n",
        "        'stats_.+_possession',\n",
        "        'stats_.+_misc'\n",
        "    ]\n",
        "    for regex in stat_table_regexes:\n",
        "        tables = soup.find_all('table', {'id': re.compile(regex)})\n",
        "        for team_idx, table in enumerate(tables):\n",
        "            team_key = 'home_' if team_idx == 0 else 'away_'\n",
        "            for row in table.select('tfoot tr'):\n",
        "                for cell in row.select('td'):\n",
        "                    if cell.has_attr('data-stat'):\n",
        "                        match[f'{team_key}{cell[\"data-stat\"]}'] = cell.text.strip()\n",
        "                        \n",
        "    # get goalkeeper stats\n",
        "    keeper_tables = soup.find_all('table', {'id': re.compile('keeper_stats_')})\n",
        "    for team_idx, table in enumerate(keeper_tables):\n",
        "        team_key = 'home_' if team_idx == 0 else 'away_'\n",
        "        row = table.select('tr')[1]\n",
        "        for cell in row.select('td')[3:]:\n",
        "            if cell.has_attr('data-stat'):\n",
        "                match[f'{team_key}{cell[\"data-stat\"]}'] = cell.text.strip()\n",
        "    \n",
        "    return match"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## An improved version for parse_match() function"
      ],
      "metadata": {
        "id": "0Njw9_SKyHLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def parse_match(html, url):\n",
        "    soup = BeautifulSoup(html, 'lxml')\n",
        "    match = {}\n",
        "\n",
        "    # get game meta data\n",
        "    league = ' '.join(re.findall('[a-zA-Z]+', url.split('-')[-2]))\n",
        "    match['match_url'] = url\n",
        "    match['league_id'] = league\n",
        "    match['game_id'] = url.split('/')[-2]\n",
        "    match['date'], match['game_time'] = soup.select_one('span.venuetime')['data-venue-date'], soup.select_one('span.venuetime')['data-venue-time']\n",
        "\n",
        "    # get team names\n",
        "    title = soup.title.string.split(' Report')[0].replace('.', '').replace(' ', '_')\n",
        "    title = title.split('_Match')[0]\n",
        "    team_names = title.split(\"_vs_\")\n",
        "    home_team, away_team = team_names[0].replace('_', \" \"), team_names[1].replace('_', \" \")\n",
        "    match['match'], match['home_team'], match['away_team'] = title, home_team, away_team\n",
        "\n",
        "    # get scores and xG\n",
        "    scorebox = soup.select_one('div.scorebox')\n",
        "    match['home_score'], match['away_score'] = scorebox.select_one('div.score').text.strip(), scorebox.select('div.score')[1].text.strip()\n",
        "    match['home_score_xg'], match['away_score_xg'] = scorebox.select_one('div.score_xg').text.strip(), scorebox.select('div.score_xg')[1].text.strip()\n",
        "\n",
        "    # get stats\n",
        "    stat_table_ids = [\n",
        "        'passing',\n",
        "        'passing_types',\n",
        "        'defense',\n",
        "        'possession',\n",
        "        'misc'\n",
        "    ]\n",
        "    for stat_table_id in stat_table_ids:\n",
        "        tables = soup.select(f'table[id$=\"{stat_table_id}\"]')\n",
        "        for team_idx, table in enumerate(tables):\n",
        "            team_key = 'home_' if team_idx == 0 else 'away_'\n",
        "            for row in table.select('tfoot tr'):\n",
        "                for cell in row.select('td'):\n",
        "                    if cell.has_attr('data-stat'):\n",
        "                        match[f'{team_key}{cell[\"data-stat\"]}'] = cell.text.strip()\n",
        "\n",
        "    # get goalkeeper stats\n",
        "    keeper_tables = soup.select('table[id^=\"keeper_stats_\"]')\n",
        "    for team_idx, table in enumerate(keeper_tables):\n",
        "        team_key = 'home_' if team_idx == 0 else 'away_'\n",
        "        row = table.select('tr')[1]\n",
        "        for cell in row.select('td')[3:]:\n",
        "            if cell.has_attr('data-stat'):\n",
        "                match[f'{team_key}{cell[\"data-stat\"]}'] = cell.text.strip()\n",
        "\n",
        "    return match"
      ],
      "metadata": {
        "id": "tGahO92tyGt1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdpY2g3bJc_u"
      },
      "source": [
        "The main changes I made include:\n",
        "\n",
        "I separated the parsing of each match into a separate function get_match_data, which takes a single match page URL as input and returns the parsed data as output. This makes it easier to parallelize the scraping process later.\n",
        "\n",
        "I added exception handling to get_match_data so that it returns None if parsing fails for any reason.\n",
        "\n",
        "I replaced the for loop that processes each match page with a concurrent.futures.ThreadPoolExecutor that spawns worker threads to process the match pages in parallel. This can greatly speed up the scraping process, especially when there are a large number of match pages to process.\n",
        "\n",
        "I added a check to skip over matches that failed to parse, so that they don't get included in the final matches list. This helps ensure that the output data is as clean and accurate as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwTqo6FsJc_x"
      },
      "source": [
        "To save the CSV file with the current date in the filename, you can use the strftime function from the datetime module to format the current date and time as a string. Here's an updated version of the code that saves the file with the current date and appends new matches to the existing file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHe0t6T1Jc_y",
        "outputId": "62fa30bc-d495-45a1-9280-0f2d5bdc8ee0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped 8 matches\n",
            "Saved updated matches to CSV file: epl_scraped_matches.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import datetime as dt\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "\n",
        "session = requests.Session()\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
        "                  '(KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
        "}\n",
        "session.headers.update(headers)\n",
        "# load the list of scraped matches from file if it exists, otherwise initialize it\n",
        "scraped_matches_file = f'/content/drive/MyDrive/colab_data/{name}_scraped_matches.txt'\n",
        "if os.path.exists(scraped_matches_file):\n",
        "    with open(scraped_matches_file, 'r') as f:\n",
        "        scraped_matches = f.read().splitlines()\n",
        "else:\n",
        "    scraped_matches = []\n",
        "\n",
        "matches = []\n",
        "urls = []\n",
        "for i in match_pages:\n",
        "  url = 'https://fbref.com' + i\n",
        "  urls.append(url)\n",
        "url_set = set(urls)\n",
        "scraped_matches_set = set(scraped_matches)\n",
        "to_scrape_urls = url_set.difference(scraped_matches_set)\n",
        "to_scrape_urls = list(to_scrape_urls)\n",
        "\n",
        "if len(to_scrape_urls) != 0:\n",
        "  for url in to_scrape_urls:\n",
        "      response = session.get(url)\n",
        "      html = response.text\n",
        "      \n",
        "      success = False\n",
        "      while not success:\n",
        "          try:\n",
        "              match = parse_match(html, url)\n",
        "              matches.append(match)\n",
        "              # append scraped matches to a list\n",
        "              scraped_matches.append(url)\n",
        "              # show that some activity is taking place\n",
        "              if len(matches)%100==0: print(len(matches), dt.datetime.now().time())\n",
        "              success = True\n",
        "          except:\n",
        "              time.sleep(3) # wait 3 seconds before trying again\n",
        "      \n",
        "      time.sleep(2) # wait 2 seconds before making another request\n",
        "\n",
        "  # save scraped URLs to file\n",
        "  with open(scraped_matches_file, 'w') as f:\n",
        "      for url in scraped_matches:\n",
        "          f.write(url + '\\n')\n",
        "\n",
        "  print(f'Scraped {len(matches)} matches')\n",
        "\n",
        "  # convert list of dictionaries to DataFrame\n",
        "  df = pd.DataFrame.from_dict(matches)\n",
        "\n",
        "  # load existing CSV file if it exists\n",
        "  csv_file = f'/content/drive/MyDrive/colab_data/{name}_scraped_matches.csv'\n",
        "  if os.path.exists(csv_file):\n",
        "      existing_df = pd.read_csv(csv_file)\n",
        "      # filter out matches that have already been scraped\n",
        "      existing_urls = set(existing_df['match_url'])\n",
        "      new_matches = df[~df['match_url'].isin(existing_urls)]\n",
        "      # append new matches to existing DataFrame\n",
        "      updated_df = pd.concat([existing_df, new_matches], ignore_index=True)\n",
        "  else:\n",
        "      updated_df = df\n",
        "\n",
        "  # save updated DataFrame to CSV file with current date in filename\n",
        "  #date_str = dt.datetime.now().strftime('%Y-%m-%d')\n",
        "  updated_df.to_csv(f'/content/drive/MyDrive/colab_data/{name}_scraped_matches.csv', index=False)\n",
        "\n",
        "  print(f'Saved updated matches to CSV file: {name}_scraped_matches.csv')\n",
        "else:\n",
        "  print(\"The matches are upto date\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ratelimiter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsxp8xkSbzI0",
        "outputId": "db47fb50-86c1-443c-cf64-e7909e87c396"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ratelimiter\n",
            "  Downloading ratelimiter-1.2.0.post0-py3-none-any.whl (6.6 kB)\n",
            "Installing collected packages: ratelimiter\n",
            "Successfully installed ratelimiter-1.2.0.post0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import datetime as dt\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "from ratelimiter import RateLimiter\n",
        "import random\n",
        "\n",
        "session = requests.Session()\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
        "                  '(KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
        "}\n",
        "session.headers.update(headers)\n",
        "# load the list of scraped matches from file if it exists, otherwise initialize it\n",
        "scraped_matches_file = f'/content/drive/MyDrive/colab_data/{name}_scraped_matches.txt'\n",
        "if os.path.exists(scraped_matches_file):\n",
        "    with open(scraped_matches_file, 'r') as f:\n",
        "        scraped_matches = f.read().splitlines()\n",
        "else:\n",
        "    scraped_matches = []\n",
        "\n",
        "matches = []\n",
        "urls = []\n",
        "for i in match_pages:\n",
        "  url = 'https://fbref.com' + i\n",
        "  urls.append(url)\n",
        "url_set = set(urls)\n",
        "scraped_matches_set = set(scraped_matches)\n",
        "to_scrape_urls = url_set.difference(scraped_matches_set)\n",
        "to_scrape_urls = list(to_scrape_urls)\n",
        "\n",
        "if len(to_scrape_urls) != 0:\n",
        "\n",
        "  # Set the maximum number of requests per second\n",
        "  rate_limiter = RateLimiter(max_calls=2, period=1)\n",
        "\n",
        "  for url in to_scrape_urls:\n",
        "      # Use the rate limiter to control the number of requests made per second\n",
        "      with rate_limiter:\n",
        "          response = session.get(url)\n",
        "          html = response.text\n",
        "\n",
        "      success = False\n",
        "      while not success:\n",
        "          try:\n",
        "              match = parse_match(html, url)\n",
        "              matches.append(match)\n",
        "              # append scraped matches to a list\n",
        "              scraped_matches.append(url)\n",
        "              # show that some activity is taking place\n",
        "              if len(matches) % 100 == 0:\n",
        "                  print(len(matches), dt.datetime.now().time())\n",
        "              success = True\n",
        "          except:\n",
        "              time.sleep(3)  # wait 3 seconds before trying again\n",
        "\n",
        "      # Add a random delay between 1 and 5 seconds\n",
        "      time.sleep(random.uniform(1, 5))\n",
        "\n",
        "  # save scraped URLs to file\n",
        "  with open(scraped_matches_file, 'w') as f:\n",
        "      for url in scraped_matches:\n",
        "          f.write(url + '\\n')\n",
        "\n",
        "  print(f'Scraped {len(matches)} matches')\n",
        "\n",
        "  # convert list of dictionaries to DataFrame\n",
        "  df = pd.DataFrame.from_dict(matches)\n",
        "\n",
        "  # load existing CSV file if it exists\n",
        "  csv_file = f'/content/drive/MyDrive/colab_data/{name}_scraped_matches.csv'\n",
        "  if os.path.exists(csv_file):\n",
        "      existing_df = pd.read_csv(csv_file)\n",
        "      # filter out matches that have already been scraped\n",
        "      existing_urls = set(existing_df['match_url'])\n",
        "      new_matches = df[~df['match_url'].isin(existing_urls)]\n",
        "      # append new matches to existing DataFrame\n",
        "      updated_df = pd.concat([existing_df, new_matches], ignore_index=True)\n",
        "  else:\n",
        "      updated_df = df\n",
        "\n",
        "  # save updated DataFrame to CSV file with current date in filename\n",
        "  #date_str = dt.datetime.now().strftime('%Y-%m-%d')\n",
        "  updated_df.to_csv(f'/content/drive/MyDrive/colab_data/{name}_scraped_matches.csv', index=False)\n",
        "\n",
        "  print(f'Saved updated matches to CSV file: {name}_scraped_matches.csv')\n",
        "else:\n",
        "  print(\"The matches are upto date\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "auXDLB5IbFkr",
        "outputId": "3020bc4a-e2fe-491e-a3e0-4eca739ecbd7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-65b565425374>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m               \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m               \u001b[0mmatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-3251d96ac960>\u001b[0m in \u001b[0;36mparse_match\u001b[0;34m(html, url)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'game_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'game_time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'span.venuetime'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data-venue-date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'span.venuetime'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data-venue-time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-65b565425374>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m               \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m           \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m               \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# wait 3 seconds before trying again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m       \u001b[0;31m# Add a random delay between 1 and 5 seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Speeded Up with ChatGPT(Bing)"
      ],
      "metadata": {
        "id": "heLh230SdZ9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import datetime as dt\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "from ratelimiter import RateLimiter\n",
        "import random\n",
        "import concurrent.futures\n",
        "\n",
        "session = requests.Session()\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
        "                  '(KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
        "}\n",
        "session.headers.update(headers)\n",
        "# load the list of scraped matches from file if it exists, otherwise initialize it\n",
        "scraped_matches_file = f'/content/drive/MyDrive/colab_data/{name}_scraped_matches.txt'\n",
        "if os.path.exists(scraped_matches_file):\n",
        "    with open(scraped_matches_file, 'r') as f:\n",
        "        scraped_matches = f.read().splitlines()\n",
        "else:\n",
        "    scraped_matches = []\n",
        "\n",
        "matches = []\n",
        "urls = []\n",
        "for i in match_pages:\n",
        "  url = 'https://fbref.com' + i\n",
        "  urls.append(url)\n",
        "url_set = set(urls)\n",
        "scraped_matches_set = set(scraped_matches)\n",
        "to_scrape_urls = url_set.difference(scraped_matches_set)\n",
        "to_scrape_urls = list(to_scrape_urls)\n",
        "\n",
        "if len(to_scrape_urls) != 0:\n",
        "\n",
        "  # Set the maximum number of requests per second\n",
        "  rate_limiter = RateLimiter(max_calls=2, period=1)\n",
        "\n",
        "  def scrape_url(url):\n",
        "      # Use the rate limiter to control the number of requests made per second\n",
        "      with rate_limiter:\n",
        "          response = session.get(url)\n",
        "          html = response.text\n",
        "\n",
        "      success = False\n",
        "      while not success:\n",
        "          try:\n",
        "              match = parse_match(html, url)\n",
        "              matches.append(match)\n",
        "              # append scraped matches to a list\n",
        "              scraped_matches.append(url)\n",
        "              # show that some activity is taking place\n",
        "              if len(matches) % 100 == 0:\n",
        "                  print(len(matches), dt.datetime.now().time())\n",
        "              success = True\n",
        "          except:\n",
        "              time.sleep(3)  # wait 3 seconds before trying again\n",
        "\n",
        "      # Add a random delay between 1 and 5 seconds\n",
        "      time.sleep(random.uniform(1, 5))\n",
        "\n",
        "  # Use a ThreadPoolExecutor to make multiple requests at the same time\n",
        "  \"\"\"\n",
        "  This code uses the concurrent.futures module to create a ThreadPoolExecutor \n",
        "  that can run multiple instances of the scrape_url function at the same time. \n",
        "  This can significantly reduce the time it takes to scrape a large number of pages.\n",
        "  \"\"\"\n",
        "  with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "      executor.map(scrape_url, to_scrape_urls)\n",
        "\n",
        "  # save scraped URLs to file\n",
        "  with open(scraped_matches_file, 'w') as f:\n",
        "      for url in scraped_matches:\n",
        "          f.write(url + '\\n')\n",
        "\n",
        "  print(f'Scraped {len(matches)} matches')\n",
        "\n",
        "  # convert list of dictionaries to DataFrame\n",
        "  df = pd.DataFrame.from_dict(matches)\n",
        "\n",
        "  # load existing CSV file if it exists\n",
        "  csv_file = f'/content/drive/MyDrive/colab_data/{name}_scraped_matches.csv'\n",
        "  if os.path.exists(csv_file):\n",
        "      existing_df = pd.read_csv(csv_file)\n",
        "      # filter out matches that have already been scraped\n",
        "      existing_urls = set(existing_df['match_url'])\n",
        "      new_matches = df[~df['match_url'].isin(existing_urls)]\n",
        "      # append new matches to existing DataFrame\n",
        "      updated_df = pd.concat([existing_df, new_matches], ignore_index=True)\n",
        "  else:\n",
        "      updated_df = df\n",
        "\n",
        "  # save updated DataFrame to CSV file with current date in filename\n",
        "  #date_str = dt.datetime.now().strftime('%Y-%m-%d')\n",
        "  updated_df.to_csv(f'/content/drive/MyDrive/colab_data/{name}_scraped_matches.csv', index=False)\n",
        "\n",
        "  print(f'Saved updated matches to CSV file: {name}_scraped_matches.csv')\n",
        "else:\n",
        "  print(\"The matches are upto date\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "AVSYXiJkdODp",
        "outputId": "91f9b4b0-461a-4fc5-9e3f-c83b27390dbf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-2018d9e5c39b>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0mThis\u001b[0m \u001b[0mcan\u001b[0m \u001b[0msignificantly\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtime\u001b[0m \u001b[0mit\u001b[0m \u001b[0mtakes\u001b[0m \u001b[0mto\u001b[0m \u001b[0mscrape\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlarge\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mpages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \"\"\"\n\u001b[0;32m---> 68\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m       \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscrape_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_scrape_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dft = pd.DataFrame(matches)\n",
        "dft.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DJfqT1w1WX3",
        "outputId": "7ccc665a-248d-4ffa-956a-dd1055406744"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32, 194)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dft.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "WPd0XB3F1iPt",
        "outputId": "4a4c8590-9406-4f5b-bf62-a0d65cc49d18"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           match_url league_id   game_id  \\\n",
              "0  https://fbref.com/en/matches/3e063ecd/Lyon-Tou...     Ligue  3e063ecd   \n",
              "1  https://fbref.com/en/matches/3e4c3c7a/Metz-Bre...     Ligue  3e4c3c7a   \n",
              "2  https://fbref.com/en/matches/fd15ca55/Strasbou...     Ligue  fd15ca55   \n",
              "3  https://fbref.com/en/matches/20f37413/Montpell...     Ligue  20f37413   \n",
              "4  https://fbref.com/en/matches/6226baab/Toulouse...     Ligue  6226baab   \n",
              "\n",
              "         date game_time                               match    home_team  \\\n",
              "0  2022-10-07     21:00                    Lyon_vs_Toulouse         Lyon   \n",
              "1  2022-04-24     15:00                       Metz_vs_Brest         Metz   \n",
              "2  2022-02-06     15:00                Strasbourg_vs_Nantes   Strasbourg   \n",
              "3  2017-09-23     17:00  Montpellier_vs_Paris_Saint-Germain  Montpellier   \n",
              "4  2023-02-01     19:00                  Toulouse_vs_Troyes     Toulouse   \n",
              "\n",
              "             away_team home_score away_score  ... away_fouls away_fouled  \\\n",
              "0             Toulouse          1          1  ...         11          13   \n",
              "1                Brest          0          1  ...         17          16   \n",
              "2               Nantes          1          0  ...          8           7   \n",
              "3  Paris Saint-Germain          0          0  ...         13          15   \n",
              "4               Troyes          4          1  ...         14          10   \n",
              "\n",
              "  away_offsides away_pens_won away_pens_conceded away_own_goals  \\\n",
              "0             1             0                  0              0   \n",
              "1             3             0                  0              0   \n",
              "2             1             0                  0              0   \n",
              "3             1             0                  0              0   \n",
              "4             0             1                  1              0   \n",
              "\n",
              "  away_ball_recoveries away_aerials_won away_aerials_lost away_aerials_won_pct  \n",
              "0                   58                9                16                 36.0  \n",
              "1                   50               15                15                 50.0  \n",
              "2                   66               19                32                 37.3  \n",
              "3                   46               13                 9                 59.1  \n",
              "4                   38               15                 5                 75.0  \n",
              "\n",
              "[5 rows x 194 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9026e1da-2ed3-4767-9935-6447fc773da7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>match_url</th>\n",
              "      <th>league_id</th>\n",
              "      <th>game_id</th>\n",
              "      <th>date</th>\n",
              "      <th>game_time</th>\n",
              "      <th>match</th>\n",
              "      <th>home_team</th>\n",
              "      <th>away_team</th>\n",
              "      <th>home_score</th>\n",
              "      <th>away_score</th>\n",
              "      <th>...</th>\n",
              "      <th>away_fouls</th>\n",
              "      <th>away_fouled</th>\n",
              "      <th>away_offsides</th>\n",
              "      <th>away_pens_won</th>\n",
              "      <th>away_pens_conceded</th>\n",
              "      <th>away_own_goals</th>\n",
              "      <th>away_ball_recoveries</th>\n",
              "      <th>away_aerials_won</th>\n",
              "      <th>away_aerials_lost</th>\n",
              "      <th>away_aerials_won_pct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://fbref.com/en/matches/3e063ecd/Lyon-Tou...</td>\n",
              "      <td>Ligue</td>\n",
              "      <td>3e063ecd</td>\n",
              "      <td>2022-10-07</td>\n",
              "      <td>21:00</td>\n",
              "      <td>Lyon_vs_Toulouse</td>\n",
              "      <td>Lyon</td>\n",
              "      <td>Toulouse</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>11</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>58</td>\n",
              "      <td>9</td>\n",
              "      <td>16</td>\n",
              "      <td>36.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://fbref.com/en/matches/3e4c3c7a/Metz-Bre...</td>\n",
              "      <td>Ligue</td>\n",
              "      <td>3e4c3c7a</td>\n",
              "      <td>2022-04-24</td>\n",
              "      <td>15:00</td>\n",
              "      <td>Metz_vs_Brest</td>\n",
              "      <td>Metz</td>\n",
              "      <td>Brest</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>17</td>\n",
              "      <td>16</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>50</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://fbref.com/en/matches/fd15ca55/Strasbou...</td>\n",
              "      <td>Ligue</td>\n",
              "      <td>fd15ca55</td>\n",
              "      <td>2022-02-06</td>\n",
              "      <td>15:00</td>\n",
              "      <td>Strasbourg_vs_Nantes</td>\n",
              "      <td>Strasbourg</td>\n",
              "      <td>Nantes</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>66</td>\n",
              "      <td>19</td>\n",
              "      <td>32</td>\n",
              "      <td>37.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://fbref.com/en/matches/20f37413/Montpell...</td>\n",
              "      <td>Ligue</td>\n",
              "      <td>20f37413</td>\n",
              "      <td>2017-09-23</td>\n",
              "      <td>17:00</td>\n",
              "      <td>Montpellier_vs_Paris_Saint-Germain</td>\n",
              "      <td>Montpellier</td>\n",
              "      <td>Paris Saint-Germain</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>13</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>46</td>\n",
              "      <td>13</td>\n",
              "      <td>9</td>\n",
              "      <td>59.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://fbref.com/en/matches/6226baab/Toulouse...</td>\n",
              "      <td>Ligue</td>\n",
              "      <td>6226baab</td>\n",
              "      <td>2023-02-01</td>\n",
              "      <td>19:00</td>\n",
              "      <td>Toulouse_vs_Troyes</td>\n",
              "      <td>Toulouse</td>\n",
              "      <td>Troyes</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>14</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "      <td>15</td>\n",
              "      <td>5</td>\n",
              "      <td>75.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  194 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9026e1da-2ed3-4767-9935-6447fc773da7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9026e1da-2ed3-4767-9935-6447fc773da7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9026e1da-2ed3-4767-9935-6447fc773da7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Another Attempt at improving speed"
      ],
      "metadata": {
        "id": "OyVoqyhvfSwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import datetime as dt\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "from ratelimiter import RateLimiter\n",
        "import random\n",
        "import concurrent.futures\n",
        "import requests_cache\n",
        "\n",
        "session = requests.Session()\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
        "                  '(KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
        "}\n",
        "session.headers.update(headers)\n",
        "# load the list of scraped matches from file if it exists, otherwise initialize it\n",
        "scraped_matches_file = f'/content/drive/MyDrive/colab_data/{name}_scraped_matches.txt'\n",
        "if os.path.exists(scraped_matches_file):\n",
        "    with open(scraped_matches_file, 'r') as f:\n",
        "        scraped_matches = f.read().splitlines()\n",
        "else:\n",
        "    scraped_matches = []\n",
        "\n",
        "matches = []\n",
        "urls = []\n",
        "for i in match_pages:\n",
        "  url = 'https://fbref.com' + i\n",
        "  urls.append(url)\n",
        "url_set = set(urls)\n",
        "scraped_matches_set = set(scraped_matches)\n",
        "to_scrape_urls = url_set.difference(scraped_matches_set)\n",
        "to_scrape_urls = list(to_scrape_urls)\n",
        "\n",
        "if len(to_scrape_urls) != 0:\n",
        "\n",
        "  # Set up a cache to store responses to requests\n",
        "  requests_cache.install_cache('scraper_cache')\n",
        "\n",
        "  # Set the maximum number of requests per second\n",
        "  rate_limiter = RateLimiter(max_calls=2, period=1)\n",
        "\n",
        "  def scrape_url(url):\n",
        "      # Use the rate limiter to control the number of requests made per second\n",
        "      with rate_limiter:\n",
        "          response = session.get(url)\n",
        "          html = response.text\n",
        "\n",
        "      success = False\n",
        "      while not success:\n",
        "          try:\n",
        "              match = parse_match(html, url)\n",
        "              matches.append(match)\n",
        "              # append scraped matches to a list\n",
        "              scraped_matches.append(url)\n",
        "              # show that some activity is taking place\n",
        "              if len(matches) % 100 == 0:\n",
        "                  print(len(matches), dt.datetime.now().time())\n",
        "              success = True\n",
        "          except:\n",
        "              time.sleep(3)  # wait 3 seconds before trying again\n",
        "\n",
        "      # Add a random delay between 1 and 5 seconds\n",
        "      time.sleep(random.uniform(1, 5))\n",
        "\n",
        "  # Use a ThreadPoolExecutor to make multiple requests at the same time\n",
        "  with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "      executor.map(scrape_url, to_scrape_urls)\n",
        "\n",
        "  # save scraped URLs to file\n",
        "  with open(scraped_matches_file, 'w') as f:\n",
        "      for url in scraped_matches:\n",
        "          f.write(url + '\\n')\n",
        "\n",
        "  print(f'Scraped {len(matches)} matches')\n",
        "\n",
        "  # convert list of dictionaries to DataFrame\n",
        "  df = pd.DataFrame.from_dict(matches)\n",
        "\n",
        "  # load existing CSV file if it exists\n",
        "  csv_file = f'/content/drive/MyDrive/colab_data/{name}_scraped_matches.csv'\n",
        "  if os.path.exists(csv_file):\n",
        "      existing_df = pd.read_csv(csv_file)\n",
        "      # filter out matches that have already been scraped\n",
        "      existing_urls = set(existing_df['match_url'])\n",
        "      new_matches = df[~df['match_url'].isin(existing_urls)]\n",
        "      # append new matches to existing DataFrame\n",
        "      updated_df = pd.concat([existing_df, new_matches], ignore_index=True)\n",
        "  else:\n",
        "      updated_df = df\n",
        "\n",
        "  # save updated DataFrame to CSV file with current date in filename\n",
        "  #date_str = dt.datetime.now().strftime('%Y-%m-%d')\n",
        "  updated_df.to_csv(f'/content/drive/MyDrive/colab_data/{name}_scraped_matches.csv', index=False)\n",
        "\n",
        "  print(f'Saved updated matches to CSV file: {name}_scraped_matches.csv')\n",
        "else:\n",
        "  print(\"The matches are upto date\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "FUwjxKD3faV_",
        "outputId": "b216037d-fde5-4658-9275-6ea180cdbd7d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-4fbd695ee2a7>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;31m# Use a ThreadPoolExecutor to make multiple requests at the same time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m       \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscrape_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_scrape_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dft = pd.DataFrame(matches)\n",
        "dft.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgtOqTLnwTdZ",
        "outputId": "a800c3f9-96d5-473f-e69a-83546926ed3d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Random time to boost speed"
      ],
      "metadata": {
        "id": "-Ioa3WGZ_vsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import datetime as dt\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "import random\n",
        "\n",
        "session = requests.Session()\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
        "                  '(KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
        "}\n",
        "session.headers.update(headers)\n",
        "\n",
        "# load the list of scraped matches from file if it exists, otherwise initialize it\n",
        "scraped_matches_file = f'/content/drive/MyDrive/colab_data/{name}scraped_matches.txt'\n",
        "if os.path.exists(scraped_matches_file):\n",
        "    with open(scraped_matches_file, 'r') as f:\n",
        "        scraped_matches = f.read().splitlines()\n",
        "else:\n",
        "    scraped_matches = []\n",
        "\n",
        "matches = []\n",
        "urls = []\n",
        "for i in match_pages:\n",
        "  url = 'https://fbref.com' + i\n",
        "  urls.append(url)\n",
        "url_set = set(urls)\n",
        "scraped_matches_set = set(scraped_matches)\n",
        "to_scrape_urls = url_set.difference(scraped_matches_set)\n",
        "to_scrape_urls = list(to_scrape_urls)\n",
        "\n",
        "if len(to_scrape_urls) != 0:\n",
        "  for url in to_scrape_urls:\n",
        "      response = session.get(url)\n",
        "      html = response.text\n",
        "      \n",
        "      success = False\n",
        "      while not success:\n",
        "          try:\n",
        "              match = parse_match(html, url)\n",
        "              matches.append(match)\n",
        "              # append scraped matches to a list\n",
        "              scraped_matches.append(url)\n",
        "              # show that some activity is taking place\n",
        "              if len(matches)%100==0: print(len(matches), dt.datetime.now().time())\n",
        "              success = True\n",
        "          except:\n",
        "              # wait for a random amount of time between 1 and 3 seconds before trying again\n",
        "              time.sleep(random.uniform(1, 3))\n",
        "\n",
        "      # wait for a random amount of time between 1 and 3 seconds before making another request\n",
        "      time.sleep(random.uniform(1, 3))\n",
        "\n",
        "  # save scraped URLs to file\n",
        "  with open(scraped_matches_file, 'w') as f:\n",
        "      for url in scraped_matches:\n",
        "          f.write(url + '\\n')\n",
        "\n",
        "  print(f'Scraped {len(matches)} matches')\n",
        "\n",
        "  # convert list of dictionaries to DataFrame\n",
        "  df = pd.DataFrame.from_dict(matches)\n",
        "\n",
        "  # load existing CSV file if it exists\n",
        "  csv_file = 'scraped_matches.csv'\n",
        "  if os.path.exists(csv_file):\n",
        "      existing_df = pd.read_csv(csv_file)\n",
        "      # filter out matches that have already been scraped\n",
        "      existing_urls = set(existing_df['match_url'])\n",
        "      new_matches = df[~df['match_url'].isin(existing_urls)]\n",
        "      # append new matches to existing DataFrame\n",
        "      updated_df = pd.concat([existing_df, new_matches], ignore_index=True)\n",
        "  else:\n",
        "      updated_df = df\n",
        "\n",
        "  # save updated DataFrame to CSV file with current date in filename\n",
        "  #date_str = dt.datetime.now().strftime('%Y-%m-%d')\n",
        "  updated_df.to_csv(f'/content/drive/MyDrive/colab_data/{name}_scraped_matches.csv', index=False)\n",
        "  print(f'Saved updated matches to CSV file: {name}_scraped_matches.csv')\n",
        "else:\n",
        "    print(\"The matches are up to date\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "gxRSJxrBv9vN",
        "outputId": "3367661b-2732-4f8a-b6e8-46a777d699bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-10a713201908>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m               \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m               \u001b[0mmatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-3251d96ac960>\u001b[0m in \u001b[0;36mparse_match\u001b[0;34m(html, url)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'game_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'game_time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'span.venuetime'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data-venue-date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'span.venuetime'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data-venue-time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-10a713201908>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m           \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m               \u001b[0;31m# wait for a random amount of time between 1 and 3 seconds before trying again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m               \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m       \u001b[0;31m# wait for a random amount of time between 1 and 3 seconds before making another request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dft = pd.DataFrame(matches)\n",
        "dft.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e0igeUt7JDt",
        "outputId": "03d05a08-0f32-4c77-f989-aee7c9a29203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7, 194)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJvVWhB9Jc_0"
      },
      "source": [
        "In this version of the code, the pd.read_csv function is used to load the existing CSV file into a DataFrame, and the ~ operator is used with the isin function to filter out matches that have already been scraped. The new matches are then appended to the existing DataFrame using the pd.concat function with the ignore_index parameter set to True.\n",
        "\n",
        "The strftime function is used to format the current date and time as a string with the format '%Y-%m-%d', which is then included in the filename of the CSV file that"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An attempt to Speed the Scraping Process"
      ],
      "metadata": {
        "id": "vTMmnSu0xVaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import datetime as dt\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "import random\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "session = requests.Session()\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
        "                  '(KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
        "}\n",
        "session.headers.update(headers)\n",
        "\n",
        "# function to scrape a single match\n",
        "def scrape_match(url):\n",
        "    response = session.get(url)\n",
        "    html = response.text\n",
        "    try:\n",
        "        match = parse_match(html, url)\n",
        "        return match\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# load the list of scraped matches from file if it exists, otherwise initialize it\n",
        "scraped_matches_file = f'/content/drive/MyDrive/colab_data/{name}scraped_matches.txt'\n",
        "if os.path.exists(scraped_matches_file):\n",
        "    with open(scraped_matches_file, 'r') as f:\n",
        "        scraped_matches = f.read().splitlines()\n",
        "else:\n",
        "    scraped_matches = []\n",
        "\n",
        "matches = []\n",
        "urls = []\n",
        "for i in match_pages:\n",
        "  url = 'https://fbref.com' + i\n",
        "  urls.append(url)\n",
        "url_set = set(urls)\n",
        "scraped_matches_set = set(scraped_matches)\n",
        "to_scrape_urls = url_set.difference(scraped_matches_set)\n",
        "to_scrape_urls = list(to_scrape_urls)\n",
        "\n",
        "if len(to_scrape_urls) != 0:\n",
        "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "        futures = [executor.submit(scrape_match, url) for url in to_scrape_urls]\n",
        "\n",
        "        for future in as_completed(futures):\n",
        "            match = future.result()\n",
        "            if match is not None:\n",
        "                matches.append(match)\n",
        "                scraped_matches.append(match['match_url'])\n",
        "                # show that some activity is taking place\n",
        "                if len(matches)%100==0: print(len(matches), dt.datetime.now().time())\n",
        "\n",
        "    # save scraped URLs to file\n",
        "    with open(scraped_matches_file, 'w') as f:\n",
        "        for url in scraped_matches:\n",
        "            f.write(url + '\\n')\n",
        "\n",
        "    print(f'Scraped {len(matches)} matches')\n",
        "\n",
        "    # convert list of dictionaries to DataFrame\n",
        "    df = pd.DataFrame.from_dict(matches)\n",
        "\n",
        "    # load existing CSV file if it exists\n",
        "    csv_file = 'scraped_matches.csv'\n",
        "    if os.path.exists(csv_file):\n",
        "        existing_df = pd.read_csv(csv_file)\n",
        "        # filter out matches that have already been scraped\n",
        "        existing_urls = set(existing_df['match_url'])\n",
        "        new_matches = df[~df['match_url'].isin(existing_urls)]\n",
        "        # append new matches to existing DataFrame\n",
        "        updated_df = pd.concat([existing_df, new_matches], ignore_index=True)\n",
        "    else:\n",
        "        updated_df = df\n",
        "\n",
        "    # save updated DataFrame to CSV file with current date in filename\n",
        "    #date_str = dt.datetime.now().strftime('%Y-%m-%d')\n",
        "    updated_df.to_csv(f'/content/drive/MyDrive/colab_data/{name}_scraped_matches.csv', index=False)\n",
        "    print(f'Saved updated matches to CSV file: {name}_scraped_matches.csv')\n",
        "else:\n",
        "    print(\"The matches are up to date\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0i7Qte16QxG",
        "outputId": "5d3924de-3eef-4f58-b78d-6feac742c560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped 0 matches\n",
            "Saved updated matches to CSV file: ligue1_scraped_matches.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try an Optimized code by ChatGPT"
      ],
      "metadata": {
        "id": "QKXMkn9pv3i8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "py9rFBiv-fdS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "d2l",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "53d8118de6dfd1121bb11202cf34328d0cf1a4474772f9a30af191caa902c810"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}