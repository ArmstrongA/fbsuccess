{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArmstrongA/fbsuccess/blob/main/optimized_scrapper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RD4KAhMEJc_Y"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "import os\n",
        "import time\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2zZFoFBJc_h",
        "outputId": "0e75d74b-112d-470b-8ced-bd2a92bad0c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 2236 matches...\n"
          ]
        }
      ],
      "source": [
        "# Full stats for fbref.com seem to start in 2017\n",
        "epl = [\"https://fbref.com/en/comps/9/schedule/Premier-League-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/9/2021-2022/schedule/2021-2022-Premier-League-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/9/2020-2021/schedule/2020-2021-Premier-League-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/9/2019-2020/schedule/2019-2020-Premier-League-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/9/2018-2019/schedule/2018-2019-Premier-League-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/9/2017-2018/schedule/2017-2018-Premier-League-Scores-and-Fixtures\"]\n",
        "\n",
        "epl_current = [\"https://fbref.com/en/comps/9/schedule/Premier-League-Scores-and-Fixtures\"]\n",
        "\n",
        "ligue1 = [\"https://fbref.com/en/comps/13/schedule/Ligue-1-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/13/2021-2022/schedule/2021-2022-Ligue-1-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/13/2020-2021/schedule/2020-2021-Ligue-1-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/13/2019-2020/schedule/2019-2020-Ligue-1-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/13/2018-2019/schedule/2018-2019-Ligue-1-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/13/2017-2018/schedule/2017-2018-Ligue-1-Scores-and-Fixtures\"]\n",
        "\n",
        "\n",
        "ligue1_current = [\"https://fbref.com/en/comps/13/schedule/Ligue-1-Scores-and-Fixtures\"]\n",
        "\n",
        "bundesliga = [\"https://fbref.com/en/comps/20/schedule/Bundesliga-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/20/2021-2022/schedule/2021-2022-Bundesliga-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/20/2020-2021/schedule/2020-2021-Bundesliga-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/20/2019-2020/schedule/2019-2020-Bundesliga-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/20/2018-2019/schedule/2018-2019-Bundesliga-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/20/2017-2018/schedule/2017-2018-Bundesliga-Scores-and-Fixtures\"]\n",
        "\n",
        "bundesliga_current = [\"https://fbref.com/en/comps/20/schedule/Bundesliga-Scores-and-Fixtures\"]\n",
        "\n",
        "seriea = [\"https://fbref.com/en/comps/11/schedule/Serie-A-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/11/2021-2022/schedule/2021-2022-Serie-A-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/11/2020-2021/schedule/2020-2021-Serie-A-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/11/2019-2020/schedule/2019-2020-Serie-A-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/11/2018-2019/schedule/2018-2019-Serie-A-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/11/2017-2018/schedule/2017-2018-Serie-A-Scores-and-Fixtures\"]\n",
        "\n",
        "seriea_current = [\"https://fbref.com/en/comps/11/schedule/Serie-A-Scores-and-Fixtures\"]\n",
        "\n",
        "laliga = [\"https://fbref.com/en/comps/12/schedule/La-Liga-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/12/2021-2022/schedule/2021-2022-La-Liga-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/12/2020-2021/schedule/2020-2021-La-Liga-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/12/2019-2020/schedule/2019-2020-La-Liga-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/12/2018-2019/schedule/2018-2019-La-Liga-Scores-and-Fixtures\",\n",
        "    \"https://fbref.com/en/comps/12/2017-2018/schedule/2017-2018-La-Liga-Scores-and-Fixtures\"]\n",
        "\n",
        "laliga_current = [\"https://fbref.com/en/comps/12/schedule/La-Liga-Scores-and-Fixtures\"]\n",
        "\n",
        "uefa_current = [\"https://fbref.com/en/comps/8/schedule/Champions-League-Scores-and-Fixtures\"]\n",
        "\n",
        "uefa = [\"https://fbref.com/en/comps/8/schedule/Champions-League-Scores-and-Fixtures\",\n",
        "        \"https://fbref.com/en/comps/8/2021-2022/schedule/2021-2022-Champions-League-Scores-and-Fixtures\",\n",
        "        \"https://fbref.com/en/comps/8/2020-2021/schedule/2020-2021-Champions-League-Scores-and-Fixtures\",\n",
        "        \"https://fbref.com/en/comps/8/2019-2020/schedule/2019-2020-Champions-League-Scores-and-Fixtures\",\n",
        "        \"https://fbref.com/en/comps/8/2018-2019/schedule/2018-2019-Champions-League-Scores-and-Fixtures\",\n",
        "        \"https://fbref.com/en/comps/8/2017-2018/schedule/2017-2018-Champions-League-Scores-and-Fixtures\"]\n",
        "\n",
        "europa = [\"https://fbref.com/en/comps/19/2022/schedule/2022-Scores-and-Fixtures\",\n",
        "        \"https://fbref.com/en/comps/19/2021-2022/schedule/2021-2022-Europa-League-Scores-and-Fixtures\",\n",
        "        \"https://fbref.com/en/comps/19/2020-2021/schedule/2020-2021-Europa-League-Scores-and-Fixtures\",\n",
        "        \"https://fbref.com/en/comps/19/2019-2020/schedule/2019-2020-Europa-League-Scores-and-Fixtures\",\n",
        "        \"https://fbref.com/en/comps/19/2018-2019/schedule/2018-2019-Europa-League-Scores-and-Fixtures\",\n",
        "        \"https://fbref.com/en/comps/19/2017-2018/schedule/2017-2018-Europa-League-Scores-and-Fixtures\"]\n",
        "\n",
        "\n",
        "# Scrape one league at a time\n",
        "\n",
        "sched_pages = ligue1\n",
        "name = 'ligue1'\n",
        "\n",
        "match_pages = set()\n",
        "for s in sched_pages:\n",
        "    html = requests.get(s).text\n",
        "    match_url_regex = \"\\/en\\/matches\\/.{8}\\/[^\\\"]+\"\n",
        "    matches = re.findall(match_url_regex, html)\n",
        "    match_pages.update(matches)\n",
        "print(f\"found {len(match_pages)} matches...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google drive"
      ],
      "metadata": {
        "id": "O77ge_hq8FwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "er9jPZwW8Evv",
        "outputId": "3cda887e-cc3e-4244-959f-0612a25566dd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KPSxGgSoJc_n"
      },
      "outputs": [],
      "source": [
        "\n",
        "def parse_match(html, url):\n",
        "    soup = BeautifulSoup(html, 'lxml')\n",
        "    match = {}\n",
        "    \n",
        "    # get game meta data\n",
        "    league = ' '.join(re.findall('[a-zA-Z]+', url.split('-')[-2]))\n",
        "    match['match_url'] = url\n",
        "    match['league_id'] = league\n",
        "    match['game_id'] = url.split('/')[-2]\n",
        "    match['date'], match['game_time'] = soup.select_one('span.venuetime')['data-venue-date'], soup.select_one('span.venuetime')['data-venue-time']\n",
        "    \n",
        "    # get team names\n",
        "    title = soup.title.string.split(' Report')[0].replace('.', '').replace(' ', '_')\n",
        "    title = title.split('_Match')[0]\n",
        "    team_names = title.split(\"_vs_\")\n",
        "    home_team, away_team = team_names[0].replace('_', \" \"), team_names[1].replace('_', \" \")\n",
        "    match['match'], match['home_team'], match['away_team'] = title, home_team, away_team\n",
        "    \n",
        "    # get scores and xG\n",
        "    scorebox = soup.select_one('div.scorebox')\n",
        "    match['home_score'], match['away_score'] = scorebox.select_one('div.score').text.strip(), scorebox.select('div.score')[1].text.strip()\n",
        "    match['home_score_xg'], match['away_score_xg'] = scorebox.select_one('div.score_xg').text.strip(), scorebox.select('div.score_xg')[1].text.strip()\n",
        "    \n",
        "    # get stats\n",
        "    stat_table_regexes = [\n",
        "        'stats_.+_passing$',\n",
        "        'stats_.+_passing_types',\n",
        "        'stats_.+_defense',\n",
        "        'stats_.+_possession',\n",
        "        'stats_.+_misc'\n",
        "    ]\n",
        "    for regex in stat_table_regexes:\n",
        "        tables = soup.find_all('table', {'id': re.compile(regex)})\n",
        "        for team_idx, table in enumerate(tables):\n",
        "            team_key = 'home_' if team_idx == 0 else 'away_'\n",
        "            for row in table.select('tfoot tr'):\n",
        "                for cell in row.select('td'):\n",
        "                    if cell.has_attr('data-stat'):\n",
        "                        match[f'{team_key}{cell[\"data-stat\"]}'] = cell.text.strip()\n",
        "                        \n",
        "    # get goalkeeper stats\n",
        "    keeper_tables = soup.find_all('table', {'id': re.compile('keeper_stats_')})\n",
        "    for team_idx, table in enumerate(keeper_tables):\n",
        "        team_key = 'home_' if team_idx == 0 else 'away_'\n",
        "        row = table.select('tr')[1]\n",
        "        for cell in row.select('td')[3:]:\n",
        "            if cell.has_attr('data-stat'):\n",
        "                match[f'{team_key}{cell[\"data-stat\"]}'] = cell.text.strip()\n",
        "    \n",
        "    return match"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdpY2g3bJc_u"
      },
      "source": [
        "The main changes I made include:\n",
        "\n",
        "I separated the parsing of each match into a separate function get_match_data, which takes a single match page URL as input and returns the parsed data as output. This makes it easier to parallelize the scraping process later.\n",
        "\n",
        "I added exception handling to get_match_data so that it returns None if parsing fails for any reason.\n",
        "\n",
        "I replaced the for loop that processes each match page with a concurrent.futures.ThreadPoolExecutor that spawns worker threads to process the match pages in parallel. This can greatly speed up the scraping process, especially when there are a large number of match pages to process.\n",
        "\n",
        "I added a check to skip over matches that failed to parse, so that they don't get included in the final matches list. This helps ensure that the output data is as clean and accurate as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwTqo6FsJc_x"
      },
      "source": [
        "To save the CSV file with the current date in the filename, you can use the strftime function from the datetime module to format the current date and time as a string. Here's an updated version of the code that saves the file with the current date and appends new matches to the existing file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHe0t6T1Jc_y",
        "outputId": "62fa30bc-d495-45a1-9280-0f2d5bdc8ee0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped 8 matches\n",
            "Saved updated matches to CSV file: epl_scraped_matches.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import datetime as dt\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "\n",
        "session = requests.Session()\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
        "                  '(KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
        "}\n",
        "session.headers.update(headers)\n",
        "# load the list of scraped matches from file if it exists, otherwise initialize it\n",
        "scraped_matches_file = f'/content/drive/MyDrive/colab_data/{name}_scraped_matches.txt'\n",
        "if os.path.exists(scraped_matches_file):\n",
        "    with open(scraped_matches_file, 'r') as f:\n",
        "        scraped_matches = f.read().splitlines()\n",
        "else:\n",
        "    scraped_matches = []\n",
        "\n",
        "matches = []\n",
        "urls = []\n",
        "for i in match_pages:\n",
        "  url = 'https://fbref.com' + i\n",
        "  urls.append(url)\n",
        "url_set = set(urls)\n",
        "scraped_matches_set = set(scraped_matches)\n",
        "to_scrape_urls = url_set.difference(scraped_matches_set)\n",
        "to_scrape_urls = list(to_scrape_urls)\n",
        "\n",
        "if len(to_scrape_urls) != 0:\n",
        "  for url in to_scrape_urls:\n",
        "      response = session.get(url)\n",
        "      html = response.text\n",
        "      \n",
        "      success = False\n",
        "      while not success:\n",
        "          try:\n",
        "              match = parse_match(html, url)\n",
        "              matches.append(match)\n",
        "              # append scraped matches to a list\n",
        "              scraped_matches.append(url)\n",
        "              # show that some activity is taking place\n",
        "              if len(matches)%100==0: print(len(matches), dt.datetime.now().time())\n",
        "              success = True\n",
        "          except:\n",
        "              time.sleep(3) # wait 3 seconds before trying again\n",
        "      \n",
        "      time.sleep(2) # wait 2 seconds before making another request\n",
        "\n",
        "  # save scraped URLs to file\n",
        "  with open(scraped_matches_file, 'w') as f:\n",
        "      for url in scraped_matches:\n",
        "          f.write(url + '\\n')\n",
        "\n",
        "  print(f'Scraped {len(matches)} matches')\n",
        "\n",
        "  # convert list of dictionaries to DataFrame\n",
        "  df = pd.DataFrame.from_dict(matches)\n",
        "\n",
        "  # load existing CSV file if it exists\n",
        "  csv_file = f'/content/drive/MyDrive/colab_data/{name}_scraped_matches.csv'\n",
        "  if os.path.exists(csv_file):\n",
        "      existing_df = pd.read_csv(csv_file)\n",
        "      # filter out matches that have already been scraped\n",
        "      existing_urls = set(existing_df['match_url'])\n",
        "      new_matches = df[~df['match_url'].isin(existing_urls)]\n",
        "      # append new matches to existing DataFrame\n",
        "      updated_df = pd.concat([existing_df, new_matches], ignore_index=True)\n",
        "  else:\n",
        "      updated_df = df\n",
        "\n",
        "  # save updated DataFrame to CSV file with current date in filename\n",
        "  #date_str = dt.datetime.now().strftime('%Y-%m-%d')\n",
        "  updated_df.to_csv(f'/content/drive/MyDrive/colab_data/{name}_scraped_matches.csv', index=False)\n",
        "\n",
        "  print(f'Saved updated matches to CSV file: {name}_scraped_matches.csv')\n",
        "else:\n",
        "  print(\"The matches are upto date\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ratelimiter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsxp8xkSbzI0",
        "outputId": "db47fb50-86c1-443c-cf64-e7909e87c396"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ratelimiter\n",
            "  Downloading ratelimiter-1.2.0.post0-py3-none-any.whl (6.6 kB)\n",
            "Installing collected packages: ratelimiter\n",
            "Successfully installed ratelimiter-1.2.0.post0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import datetime as dt\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "from ratelimiter import RateLimiter\n",
        "import random\n",
        "\n",
        "session = requests.Session()\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
        "                  '(KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
        "}\n",
        "session.headers.update(headers)\n",
        "# load the list of scraped matches from file if it exists, otherwise initialize it\n",
        "scraped_matches_file = f'/content/drive/MyDrive/colab_data/{name}_scraped_matches.txt'\n",
        "if os.path.exists(scraped_matches_file):\n",
        "    with open(scraped_matches_file, 'r') as f:\n",
        "        scraped_matches = f.read().splitlines()\n",
        "else:\n",
        "    scraped_matches = []\n",
        "\n",
        "matches = []\n",
        "urls = []\n",
        "for i in match_pages:\n",
        "  url = 'https://fbref.com' + i\n",
        "  urls.append(url)\n",
        "url_set = set(urls)\n",
        "scraped_matches_set = set(scraped_matches)\n",
        "to_scrape_urls = url_set.difference(scraped_matches_set)\n",
        "to_scrape_urls = list(to_scrape_urls)\n",
        "\n",
        "if len(to_scrape_urls) != 0:\n",
        "\n",
        "  # Set the maximum number of requests per second\n",
        "  rate_limiter = RateLimiter(max_calls=2, period=1)\n",
        "\n",
        "  for url in to_scrape_urls:\n",
        "      # Use the rate limiter to control the number of requests made per second\n",
        "      with rate_limiter:\n",
        "          response = session.get(url)\n",
        "          html = response.text\n",
        "\n",
        "      success = False\n",
        "      while not success:\n",
        "          try:\n",
        "              match = parse_match(html, url)\n",
        "              matches.append(match)\n",
        "              # append scraped matches to a list\n",
        "              scraped_matches.append(url)\n",
        "              # show that some activity is taking place\n",
        "              if len(matches) % 100 == 0:\n",
        "                  print(len(matches), dt.datetime.now().time())\n",
        "              success = True\n",
        "          except:\n",
        "              time.sleep(3)  # wait 3 seconds before trying again\n",
        "\n",
        "      # Add a random delay between 1 and 5 seconds\n",
        "      time.sleep(random.uniform(1, 5))\n",
        "\n",
        "  # save scraped URLs to file\n",
        "  with open(scraped_matches_file, 'w') as f:\n",
        "      for url in scraped_matches:\n",
        "          f.write(url + '\\n')\n",
        "\n",
        "  print(f'Scraped {len(matches)} matches')\n",
        "\n",
        "  # convert list of dictionaries to DataFrame\n",
        "  df = pd.DataFrame.from_dict(matches)\n",
        "\n",
        "  # load existing CSV file if it exists\n",
        "  csv_file = f'/content/drive/MyDrive/colab_data/{name}_scraped_matches.csv'\n",
        "  if os.path.exists(csv_file):\n",
        "      existing_df = pd.read_csv(csv_file)\n",
        "      # filter out matches that have already been scraped\n",
        "      existing_urls = set(existing_df['match_url'])\n",
        "      new_matches = df[~df['match_url'].isin(existing_urls)]\n",
        "      # append new matches to existing DataFrame\n",
        "      updated_df = pd.concat([existing_df, new_matches], ignore_index=True)\n",
        "  else:\n",
        "      updated_df = df\n",
        "\n",
        "  # save updated DataFrame to CSV file with current date in filename\n",
        "  #date_str = dt.datetime.now().strftime('%Y-%m-%d')\n",
        "  updated_df.to_csv(f'/content/drive/MyDrive/colab_data/{name}_scraped_matches.csv', index=False)\n",
        "\n",
        "  print(f'Saved updated matches to CSV file: {name}_scraped_matches.csv')\n",
        "else:\n",
        "  print(\"The matches are upto date\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "auXDLB5IbFkr",
        "outputId": "3020bc4a-e2fe-491e-a3e0-4eca739ecbd7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-65b565425374>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m               \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m               \u001b[0mmatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-3251d96ac960>\u001b[0m in \u001b[0;36mparse_match\u001b[0;34m(html, url)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'game_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'game_time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'span.venuetime'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data-venue-date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'span.venuetime'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data-venue-time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-65b565425374>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m               \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m           \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m               \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# wait 3 seconds before trying again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m       \u001b[0;31m# Add a random delay between 1 and 5 seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Speeded Up with ChatGPT(Bing)"
      ],
      "metadata": {
        "id": "heLh230SdZ9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import datetime as dt\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "from ratelimiter import RateLimiter\n",
        "import random\n",
        "import concurrent.futures\n",
        "\n",
        "session = requests.Session()\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
        "                  '(KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
        "}\n",
        "session.headers.update(headers)\n",
        "# load the list of scraped matches from file if it exists, otherwise initialize it\n",
        "scraped_matches_file = f'/content/drive/MyDrive/colab_data/{name}_scraped_matches.txt'\n",
        "if os.path.exists(scraped_matches_file):\n",
        "    with open(scraped_matches_file, 'r') as f:\n",
        "        scraped_matches = f.read().splitlines()\n",
        "else:\n",
        "    scraped_matches = []\n",
        "\n",
        "matches = []\n",
        "urls = []\n",
        "for i in match_pages:\n",
        "  url = 'https://fbref.com' + i\n",
        "  urls.append(url)\n",
        "url_set = set(urls)\n",
        "scraped_matches_set = set(scraped_matches)\n",
        "to_scrape_urls = url_set.difference(scraped_matches_set)\n",
        "to_scrape_urls = list(to_scrape_urls)\n",
        "\n",
        "if len(to_scrape_urls) != 0:\n",
        "\n",
        "  # Set the maximum number of requests per second\n",
        "  rate_limiter = RateLimiter(max_calls=2, period=1)\n",
        "\n",
        "  def scrape_url(url):\n",
        "      # Use the rate limiter to control the number of requests made per second\n",
        "      with rate_limiter:\n",
        "          response = session.get(url)\n",
        "          html = response.text\n",
        "\n",
        "      success = False\n",
        "      while not success:\n",
        "          try:\n",
        "              match = parse_match(html, url)\n",
        "              matches.append(match)\n",
        "              # append scraped matches to a list\n",
        "              scraped_matches.append(url)\n",
        "              # show that some activity is taking place\n",
        "              if len(matches) % 100 == 0:\n",
        "                  print(len(matches), dt.datetime.now().time())\n",
        "              success = True\n",
        "          except:\n",
        "              time.sleep(3)  # wait 3 seconds before trying again\n",
        "\n",
        "      # Add a random delay between 1 and 5 seconds\n",
        "      time.sleep(random.uniform(1, 5))\n",
        "\n",
        "  # Use a ThreadPoolExecutor to make multiple requests at the same time\n",
        "  \"\"\"\n",
        "  This code uses the concurrent.futures module to create a ThreadPoolExecutor \n",
        "  that can run multiple instances of the scrape_url function at the same time. \n",
        "  This can significantly reduce the time it takes to scrape a large number of pages.\n",
        "  \"\"\"\n",
        "  with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "      executor.map(scrape_url, to_scrape_urls)\n",
        "\n",
        "  # save scraped URLs to file\n",
        "  with open(scraped_matches_file, 'w') as f:\n",
        "      for url in scraped_matches:\n",
        "          f.write(url + '\\n')\n",
        "\n",
        "  print(f'Scraped {len(matches)} matches')\n",
        "\n",
        "  # convert list of dictionaries to DataFrame\n",
        "  df = pd.DataFrame.from_dict(matches)\n",
        "\n",
        "  # load existing CSV file if it exists\n",
        "  csv_file = f'/content/drive/MyDrive/colab_data/{name}_scraped_matches.csv'\n",
        "  if os.path.exists(csv_file):\n",
        "      existing_df = pd.read_csv(csv_file)\n",
        "      # filter out matches that have already been scraped\n",
        "      existing_urls = set(existing_df['match_url'])\n",
        "      new_matches = df[~df['match_url'].isin(existing_urls)]\n",
        "      # append new matches to existing DataFrame\n",
        "      updated_df = pd.concat([existing_df, new_matches], ignore_index=True)\n",
        "  else:\n",
        "      updated_df = df\n",
        "\n",
        "  # save updated DataFrame to CSV file with current date in filename\n",
        "  #date_str = dt.datetime.now().strftime('%Y-%m-%d')\n",
        "  updated_df.to_csv(f'/content/drive/MyDrive/colab_data/{name}_scraped_matches.csv', index=False)\n",
        "\n",
        "  print(f'Saved updated matches to CSV file: {name}_scraped_matches.csv')\n",
        "else:\n",
        "  print(\"The matches are upto date\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "AVSYXiJkdODp",
        "outputId": "4f9c6d67-6674-45cd-b458-6ef4d89c0dbf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-f83d6acb39ec>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m   \u001b[0;31m# Use a ThreadPoolExecutor to make multiple requests at the same time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m       \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscrape_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_scrape_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests_cache"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mq8dIPue3mR",
        "outputId": "9518862d-a409-40df-8718-6d7ee49fd117"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting requests_cache\n",
            "  Downloading requests_cache-1.0.1-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.7/58.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=21.2 in /usr/local/lib/python3.10/dist-packages (from requests_cache) (23.1.0)\n",
            "Collecting cattrs>=22.2 (from requests_cache)\n",
            "  Downloading cattrs-22.2.0-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests_cache) (3.3.0)\n",
            "Requirement already satisfied: requests>=2.22 in /usr/local/lib/python3.10/dist-packages (from requests_cache) (2.27.1)\n",
            "Collecting url-normalize>=1.4 (from requests_cache)\n",
            "  Downloading url_normalize-1.4.3-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: urllib3>=1.25.5 in /usr/local/lib/python3.10/dist-packages (from requests_cache) (1.26.15)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from cattrs>=22.2->requests_cache) (1.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22->requests_cache) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22->requests_cache) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22->requests_cache) (3.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from url-normalize>=1.4->requests_cache) (1.16.0)\n",
            "Installing collected packages: url-normalize, cattrs, requests_cache\n",
            "Successfully installed cattrs-22.2.0 requests_cache-1.0.1 url-normalize-1.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Another Attempt at improving speed"
      ],
      "metadata": {
        "id": "OyVoqyhvfSwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import datetime as dt\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "from ratelimiter import RateLimiter\n",
        "import random\n",
        "import concurrent.futures\n",
        "import requests_cache\n",
        "\n",
        "session = requests.Session()\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
        "                  '(KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
        "}\n",
        "session.headers.update(headers)\n",
        "# load the list of scraped matches from file if it exists, otherwise initialize it\n",
        "scraped_matches_file = f'/content/drive/MyDrive/colab_data/{name}_scraped_matches.txt'\n",
        "if os.path.exists(scraped_matches_file):\n",
        "    with open(scraped_matches_file, 'r') as f:\n",
        "        scraped_matches = f.read().splitlines()\n",
        "else:\n",
        "    scraped_matches = []\n",
        "\n",
        "matches = []\n",
        "urls = []\n",
        "for i in match_pages:\n",
        "  url = 'https://fbref.com' + i\n",
        "  urls.append(url)\n",
        "url_set = set(urls)\n",
        "scraped_matches_set = set(scraped_matches)\n",
        "to_scrape_urls = url_set.difference(scraped_matches_set)\n",
        "to_scrape_urls = list(to_scrape_urls)\n",
        "\n",
        "if len(to_scrape_urls) != 0:\n",
        "\n",
        "  # Set up a cache to store responses to requests\n",
        "  requests_cache.install_cache('scraper_cache')\n",
        "\n",
        "  # Set the maximum number of requests per second\n",
        "  rate_limiter = RateLimiter(max_calls=2, period=1)\n",
        "\n",
        "  def scrape_url(url):\n",
        "      # Use the rate limiter to control the number of requests made per second\n",
        "      with rate_limiter:\n",
        "          response = session.get(url)\n",
        "          html = response.text\n",
        "\n",
        "      success = False\n",
        "      while not success:\n",
        "          try:\n",
        "              match = parse_match(html, url)\n",
        "              matches.append(match)\n",
        "              # append scraped matches to a list\n",
        "              scraped_matches.append(url)\n",
        "              # show that some activity is taking place\n",
        "              if len(matches) % 100 == 0:\n",
        "                  print(len(matches), dt.datetime.now().time())\n",
        "              success = True\n",
        "          except:\n",
        "              time.sleep(3)  # wait 3 seconds before trying again\n",
        "\n",
        "      # Add a random delay between 1 and 5 seconds\n",
        "      time.sleep(random.uniform(1, 5))\n",
        "\n",
        "  # Use a ThreadPoolExecutor to make multiple requests at the same time\n",
        "  with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "      executor.map(scrape_url, to_scrape_urls)\n",
        "\n",
        "  # save scraped URLs to file\n",
        "  with open(scraped_matches_file, 'w') as f:\n",
        "      for url in scraped_matches:\n",
        "          f.write(url + '\\n')\n",
        "\n",
        "  print(f'Scraped {len(matches)} matches')\n",
        "\n",
        "  # convert list of dictionaries to DataFrame\n",
        "  df = pd.DataFrame.from_dict(matches)\n",
        "\n",
        "  # load existing CSV file if it exists\n",
        "  csv_file = f'/content/drive/MyDrive/colab_data/{name}_scraped_matches.csv'\n",
        "  if os.path.exists(csv_file):\n",
        "      existing_df = pd.read_csv(csv_file)\n",
        "      # filter out matches that have already been scraped\n",
        "      existing_urls = set(existing_df['match_url'])\n",
        "      new_matches = df[~df['match_url'].isin(existing_urls)]\n",
        "      # append new matches to existing DataFrame\n",
        "      updated_df = pd.concat([existing_df, new_matches], ignore_index=True)\n",
        "  else:\n",
        "      updated_df = df\n",
        "\n",
        "  # save updated DataFrame to CSV file with current date in filename\n",
        "  #date_str = dt.datetime.now().strftime('%Y-%m-%d')\n",
        "  updated_df.to_csv(f'/content/drive/MyDrive/colab_data/{name}_scraped_matches.csv', index=False)\n",
        "\n",
        "  print(f'Saved updated matches to CSV file: {name}_scraped_matches.csv')\n",
        "else:\n",
        "  print(\"The matches are upto date\")"
      ],
      "metadata": {
        "id": "FUwjxKD3faV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Random time to boost speed"
      ],
      "metadata": {
        "id": "-Ioa3WGZ_vsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import datetime as dt\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "import random\n",
        "\n",
        "session = requests.Session()\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
        "                  '(KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
        "}\n",
        "session.headers.update(headers)\n",
        "\n",
        "# load the list of scraped matches from file if it exists, otherwise initialize it\n",
        "scraped_matches_file = f'/content/drive/MyDrive/colab_data/{name}scraped_matches.txt'\n",
        "if os.path.exists(scraped_matches_file):\n",
        "    with open(scraped_matches_file, 'r') as f:\n",
        "        scraped_matches = f.read().splitlines()\n",
        "else:\n",
        "    scraped_matches = []\n",
        "\n",
        "matches = []\n",
        "urls = []\n",
        "for i in match_pages:\n",
        "  url = 'https://fbref.com' + i\n",
        "  urls.append(url)\n",
        "url_set = set(urls)\n",
        "scraped_matches_set = set(scraped_matches)\n",
        "to_scrape_urls = url_set.difference(scraped_matches_set)\n",
        "to_scrape_urls = list(to_scrape_urls)\n",
        "\n",
        "if len(to_scrape_urls) != 0:\n",
        "  for url in to_scrape_urls:\n",
        "      response = session.get(url)\n",
        "      html = response.text\n",
        "      \n",
        "      success = False\n",
        "      while not success:\n",
        "          try:\n",
        "              match = parse_match(html, url)\n",
        "              matches.append(match)\n",
        "              # append scraped matches to a list\n",
        "              scraped_matches.append(url)\n",
        "              # show that some activity is taking place\n",
        "              if len(matches)%100==0: print(len(matches), dt.datetime.now().time())\n",
        "              success = True\n",
        "          except:\n",
        "              # wait for a random amount of time between 1 and 3 seconds before trying again\n",
        "              time.sleep(random.uniform(1, 3))\n",
        "\n",
        "      # wait for a random amount of time between 1 and 3 seconds before making another request\n",
        "      time.sleep(random.uniform(1, 3))\n",
        "\n",
        "  # save scraped URLs to file\n",
        "  with open(scraped_matches_file, 'w') as f:\n",
        "      for url in scraped_matches:\n",
        "          f.write(url + '\\n')\n",
        "\n",
        "  print(f'Scraped {len(matches)} matches')\n",
        "\n",
        "  # convert list of dictionaries to DataFrame\n",
        "  df = pd.DataFrame.from_dict(matches)\n",
        "\n",
        "  # load existing CSV file if it exists\n",
        "  csv_file = 'scraped_matches.csv'\n",
        "  if os.path.exists(csv_file):\n",
        "      existing_df = pd.read_csv(csv_file)\n",
        "      # filter out matches that have already been scraped\n",
        "      existing_urls = set(existing_df['match_url'])\n",
        "      new_matches = df[~df['match_url'].isin(existing_urls)]\n",
        "      # append new matches to existing DataFrame\n",
        "      updated_df = pd.concat([existing_df, new_matches], ignore_index=True)\n",
        "  else:\n",
        "      updated_df = df\n",
        "\n",
        "  # save updated DataFrame to CSV file with current date in filename\n",
        "  #date_str = dt.datetime.now().strftime('%Y-%m-%d')\n",
        "  updated_df.to_csv(f'/content/drive/MyDrive/colab_data/{name}_scraped_matches.csv', index=False)\n",
        "  print(f'Saved updated matches to CSV file: {name}_scraped_matches.csv')\n",
        "else:\n",
        "    print(\"The matches are up to date\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "gxRSJxrBv9vN",
        "outputId": "3367661b-2732-4f8a-b6e8-46a777d699bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-10a713201908>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m               \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m               \u001b[0mmatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-3251d96ac960>\u001b[0m in \u001b[0;36mparse_match\u001b[0;34m(html, url)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'game_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'game_time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'span.venuetime'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data-venue-date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'span.venuetime'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data-venue-time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-10a713201908>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m           \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m               \u001b[0;31m# wait for a random amount of time between 1 and 3 seconds before trying again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m               \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m       \u001b[0;31m# wait for a random amount of time between 1 and 3 seconds before making another request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dft = pd.DataFrame(matches)\n",
        "dft.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e0igeUt7JDt",
        "outputId": "03d05a08-0f32-4c77-f989-aee7c9a29203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7, 194)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJvVWhB9Jc_0"
      },
      "source": [
        "In this version of the code, the pd.read_csv function is used to load the existing CSV file into a DataFrame, and the ~ operator is used with the isin function to filter out matches that have already been scraped. The new matches are then appended to the existing DataFrame using the pd.concat function with the ignore_index parameter set to True.\n",
        "\n",
        "The strftime function is used to format the current date and time as a string with the format '%Y-%m-%d', which is then included in the filename of the CSV file that"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An attempt to Speed the Scraping Process"
      ],
      "metadata": {
        "id": "vTMmnSu0xVaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import datetime as dt\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "import random\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "session = requests.Session()\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
        "                  '(KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
        "}\n",
        "session.headers.update(headers)\n",
        "\n",
        "# function to scrape a single match\n",
        "def scrape_match(url):\n",
        "    response = session.get(url)\n",
        "    html = response.text\n",
        "    try:\n",
        "        match = parse_match(html, url)\n",
        "        return match\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# load the list of scraped matches from file if it exists, otherwise initialize it\n",
        "scraped_matches_file = f'/content/drive/MyDrive/colab_data/{name}scraped_matches.txt'\n",
        "if os.path.exists(scraped_matches_file):\n",
        "    with open(scraped_matches_file, 'r') as f:\n",
        "        scraped_matches = f.read().splitlines()\n",
        "else:\n",
        "    scraped_matches = []\n",
        "\n",
        "matches = []\n",
        "urls = []\n",
        "for i in match_pages:\n",
        "  url = 'https://fbref.com' + i\n",
        "  urls.append(url)\n",
        "url_set = set(urls)\n",
        "scraped_matches_set = set(scraped_matches)\n",
        "to_scrape_urls = url_set.difference(scraped_matches_set)\n",
        "to_scrape_urls = list(to_scrape_urls)\n",
        "\n",
        "if len(to_scrape_urls) != 0:\n",
        "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "        futures = [executor.submit(scrape_match, url) for url in to_scrape_urls]\n",
        "\n",
        "        for future in as_completed(futures):\n",
        "            match = future.result()\n",
        "            if match is not None:\n",
        "                matches.append(match)\n",
        "                scraped_matches.append(match['match_url'])\n",
        "                # show that some activity is taking place\n",
        "                if len(matches)%100==0: print(len(matches), dt.datetime.now().time())\n",
        "\n",
        "    # save scraped URLs to file\n",
        "    with open(scraped_matches_file, 'w') as f:\n",
        "        for url in scraped_matches:\n",
        "            f.write(url + '\\n')\n",
        "\n",
        "    print(f'Scraped {len(matches)} matches')\n",
        "\n",
        "    # convert list of dictionaries to DataFrame\n",
        "    df = pd.DataFrame.from_dict(matches)\n",
        "\n",
        "    # load existing CSV file if it exists\n",
        "    csv_file = 'scraped_matches.csv'\n",
        "    if os.path.exists(csv_file):\n",
        "        existing_df = pd.read_csv(csv_file)\n",
        "        # filter out matches that have already been scraped\n",
        "        existing_urls = set(existing_df['match_url'])\n",
        "        new_matches = df[~df['match_url'].isin(existing_urls)]\n",
        "        # append new matches to existing DataFrame\n",
        "        updated_df = pd.concat([existing_df, new_matches], ignore_index=True)\n",
        "    else:\n",
        "        updated_df = df\n",
        "\n",
        "    # save updated DataFrame to CSV file with current date in filename\n",
        "    #date_str = dt.datetime.now().strftime('%Y-%m-%d')\n",
        "    updated_df.to_csv(f'/content/drive/MyDrive/colab_data/{name}_scraped_matches.csv', index=False)\n",
        "    print(f'Saved updated matches to CSV file: {name}_scraped_matches.csv')\n",
        "else:\n",
        "    print(\"The matches are up to date\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0i7Qte16QxG",
        "outputId": "5d3924de-3eef-4f58-b78d-6feac742c560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped 0 matches\n",
            "Saved updated matches to CSV file: ligue1_scraped_matches.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try an Optimized code by ChatGPT"
      ],
      "metadata": {
        "id": "QKXMkn9pv3i8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "py9rFBiv-fdS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "d2l",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "53d8118de6dfd1121bb11202cf34328d0cf1a4474772f9a30af191caa902c810"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}